{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_paOLTSrwb00",
        "outputId": "a7632fd2-1ff6-41fe-83ec-e38595ae1a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate transformers diffusers torch torchvision pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNMPuZUJwQfD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from diffusers import StableVideoDiffusionPipeline, AutoencoderKL, ControlNetModel, DDPMScheduler\n",
        "from diffusers.models import UNetSpatioTemporalConditionModel\n",
        "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from torchvision import transforms\n",
        "from accelerate import Accelerator\n",
        "from typing import Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqwiK0GlykRE"
      },
      "outputs": [],
      "source": [
        "# ----------------- 1. Configuration and Setup -----------------\n",
        "\n",
        "MODEL_ID = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
        "DATASET_ROOT = \"/content/data/rovi-reduc\"  # Fixed: Consistent path\n",
        "RESOLUTION = 400\n",
        "BATCH_SIZE = 1\n",
        "NUM_FRAMES = 8\n",
        "NUM_EPOCHS = 10\n",
        "GRADIENT_ACCUMULATION_STEPS = 2\n",
        "CHECKPOINT_SAVE_EPOCHS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6E0DgFPyw8Q",
        "outputId": "fd45def3-d61a-4e4f-f310-4a896d61ea77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Initialize Accelerator\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    mixed_precision='fp16'\n",
        ")\n",
        "device = accelerator.device\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAYZseTBzd0Y"
      },
      "outputs": [],
      "source": [
        "# ----------------- 2. Dataset Implementation -----------------\n",
        "\n",
        "class RoVITrackingInpaintingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Video tracking + inpainting dataset.\n",
        "    Input: Original video + First frame mask only\n",
        "    Target: Edited video + All frame masks\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path, sequence_length=8, image_size=(256, 256)):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.jpeg_dir = self.dataset_path / 'JPEGImages'\n",
        "        self.inpaint_dir = self.dataset_path / 'InpaintImages'\n",
        "        self.annotations_dir = self.dataset_path / 'Annotations'\n",
        "        self.sequence_length = sequence_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "        if not self.jpeg_dir.exists():\n",
        "            raise ValueError(f\"JPEGImages not found\")\n",
        "        if not self.inpaint_dir.exists():\n",
        "            raise ValueError(f\"InpaintImages not found\")\n",
        "        if not self.annotations_dir.exists():\n",
        "            raise ValueError(f\"Annotations not found\")\n",
        "\n",
        "        all_sequences = sorted([p.name for p in self.jpeg_dir.iterdir() if p.is_dir()])\n",
        "        self.sequences = [\n",
        "            seq for seq in all_sequences\n",
        "            if (self.inpaint_dir / seq / '1').is_dir()\n",
        "            and (self.annotations_dir / seq).is_dir()\n",
        "        ]\n",
        "\n",
        "        if len(self.sequences) == 0:\n",
        "            raise ValueError(f\"No valid sequences found\")\n",
        "\n",
        "        print(f\"Found {len(self.sequences)} sequences for tracking + inpainting\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
        "        ])\n",
        "\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_name = self.sequences[idx]\n",
        "\n",
        "        original_frames_path = self.jpeg_dir / video_name\n",
        "        edited_frames_path = self.inpaint_dir / video_name / '1'\n",
        "        mask_frames_path = self.annotations_dir / video_name\n",
        "\n",
        "        frame_files = sorted([f.name for f in original_frames_path.glob('*.jpg')])\n",
        "        frame_files = frame_files[:self.sequence_length]\n",
        "\n",
        "        if len(frame_files) < self.sequence_length:\n",
        "            frame_files = frame_files + [frame_files[-1]] * (self.sequence_length - len(frame_files))\n",
        "\n",
        "        original_video = []\n",
        "        edited_video = []\n",
        "        mask_video = []\n",
        "\n",
        "        for filename in frame_files:\n",
        "            # ORIGINAL (input)\n",
        "            original_img = Image.open(original_frames_path / filename).convert(\"RGB\")\n",
        "            original_video.append(self.transform(original_img))\n",
        "\n",
        "            # EDITED (target for inpainting)\n",
        "            edited_img = Image.open(edited_frames_path / filename).convert(\"RGB\")\n",
        "            edited_video.append(self.transform(edited_img))\n",
        "\n",
        "            # MASK (target for tracking)\n",
        "            mask_filename = filename.replace('.jpg', '.png')\n",
        "            try:\n",
        "                mask_img = Image.open(mask_frames_path / mask_filename).convert(\"L\")\n",
        "            except FileNotFoundError:\n",
        "                mask_img = Image.open(mask_frames_path / filename).convert(\"L\")\n",
        "\n",
        "            mask_tensor = self.mask_transform(mask_img)\n",
        "            mask_video.append((mask_tensor > 0.1).float())\n",
        "\n",
        "        # Stack and permute to (C, T, H, W)\n",
        "        original_video = torch.stack(original_video).permute(1, 0, 2, 3)\n",
        "        edited_video = torch.stack(edited_video).permute(1, 0, 2, 3)\n",
        "        mask_video = torch.stack(mask_video).permute(1, 0, 2, 3)\n",
        "\n",
        "        # First frames\n",
        "        original_first_frame = original_video[:, 0, :, :]\n",
        "        first_mask = mask_video[:, 0:1, :, :]  # Only first mask (1, H, W)\n",
        "\n",
        "        return {\n",
        "            \"original_video\": original_video,\n",
        "            \"original_first_frame\": original_first_frame,\n",
        "            \"first_mask\": first_mask,           # INPUT: only first mask\n",
        "            \"edited_video\": edited_video,        # TARGET: inpainted video\n",
        "            \"mask_video\": mask_video,            # TARGET: all masks for tracking\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hhM925nBzdyY"
      },
      "outputs": [],
      "source": [
        "# ----------------- 3. Model Components -----------------\n",
        "\n",
        "class SelectiveContentEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    SCE: Encodes first frame mask into features.\n",
        "    Input: First frame mask (B, 1, H, W)\n",
        "    Output: Encoded features (B, feature_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, feature_dim),\n",
        "        )\n",
        "        print(f\"SCE initialized with {feature_dim} feature dimensions\")\n",
        "\n",
        "    def forward(self, first_mask):\n",
        "        \"\"\"first_mask: (B, 1, H, W) -> features: (B, feature_dim)\"\"\"\n",
        "        return self.encoder(first_mask)\n",
        "\n",
        "class ControlAdapter3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Control Adapter: Processes mask video for inpainting guidance.\n",
        "    Input: Mask video (B, 1, T, H, W)\n",
        "    Output: Control features (B, 4, T, H', W')\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, base_channels=32, latent_channels=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_in = nn.Conv3d(in_channels, base_channels, kernel_size=3, padding=1)\n",
        "        self.to_latent = nn.Conv3d(base_channels, latent_channels, kernel_size=1)\n",
        "        self.zero_conv = nn.Conv3d(latent_channels, latent_channels, kernel_size=1)\n",
        "\n",
        "        nn.init.zeros_(self.zero_conv.weight)\n",
        "        nn.init.zeros_(self.zero_conv.bias)\n",
        "\n",
        "        print(f\"ControlAdapter3D: {base_channels} base -> {latent_channels} latent channels\")\n",
        "\n",
        "    def forward(self, mask_video):\n",
        "        h = self.conv_in(mask_video)\n",
        "        h = self.to_latent(h)\n",
        "        h = self.zero_conv(h)\n",
        "        return h\n",
        "\n",
        "class MaskPredictionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MPD: Predicts masks for all frames from latent features.\n",
        "    Input: Latent features (B, 4, T, H, W) + SCE features\n",
        "    Output: Predicted masks (B, 1, T, H, W)\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_channels=4, sce_feature_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project SCE features to spatial\n",
        "        self.sce_projection = nn.Sequential(\n",
        "            nn.Linear(sce_feature_dim, 32 * 32),\n",
        "            nn.Unflatten(1, (1, 32, 32)),\n",
        "        )\n",
        "\n",
        "        # Combine latents + SCE features -> masks\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv3d(latent_channels + 1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(32, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(16, 1, kernel_size=1),\n",
        "            nn.Sigmoid(),  # Output binary masks\n",
        "        )\n",
        "        print(\"MPD initialized\")\n",
        "\n",
        "    def forward(self, latent_features, sce_features, num_frames):\n",
        "        \"\"\"\n",
        "        latent_features: (B, 4, T, H, W)\n",
        "        sce_features: (B, feature_dim)\n",
        "        -> predicted_masks: (B, 1, T, H, W)\n",
        "        \"\"\"\n",
        "        B = latent_features.shape[0]\n",
        "\n",
        "        # Project SCE to spatial and expand to all frames\n",
        "        sce_spatial = self.sce_projection(sce_features)  # (B, 1, H, W)\n",
        "        sce_spatial = F.interpolate(sce_spatial, size=latent_features.shape[-2:], mode='bilinear')\n",
        "        sce_spatial = sce_spatial.unsqueeze(2).repeat(1, 1, num_frames, 1, 1)  # (B, 1, T, H, W)\n",
        "\n",
        "        # Concatenate and decode\n",
        "        combined = torch.cat([latent_features, sce_spatial], dim=1)  # (B, 5, T, H, W)\n",
        "        masks = self.decoder(combined)  # (B, 1, T, H, W)\n",
        "\n",
        "        return masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2u57sWzdwI",
        "outputId": "c1b6e3e6-8bc3-4483-d2aa-e746b3d808c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at stabilityai/stable-video-diffusion-img2vid-xt were not used when initializing AutoencoderKL: \n",
            " ['decoder.up_blocks.1.resnets.2.temporal_res_block.norm2.bias, decoder.up_blocks.1.resnets.0.temporal_res_block.norm2.bias, decoder.up_blocks.2.resnets.0.temporal_res_block.norm1.bias, decoder.mid_block.resnets.0.spatial_res_block.conv2.weight, decoder.up_blocks.2.resnets.2.temporal_res_block.conv2.bias, decoder.up_blocks.1.resnets.1.temporal_res_block.norm1.weight, decoder.up_blocks.3.resnets.1.spatial_res_block.conv2.weight, decoder.up_blocks.2.resnets.1.spatial_res_block.conv1.bias, decoder.up_blocks.1.resnets.2.temporal_res_block.norm1.weight, decoder.up_blocks.2.resnets.0.temporal_res_block.conv2.bias, decoder.mid_block.resnets.0.time_mixer.mix_factor, decoder.up_blocks.3.resnets.1.temporal_res_block.norm1.bias, decoder.up_blocks.0.resnets.0.temporal_res_block.conv1.weight, decoder.up_blocks.0.resnets.1.temporal_res_block.norm1.bias, decoder.up_blocks.3.resnets.1.temporal_res_block.norm2.bias, decoder.up_blocks.0.resnets.2.temporal_res_block.conv2.weight, decoder.up_blocks.2.resnets.0.spatial_res_block.conv2.weight, decoder.up_blocks.1.upsamplers.0.conv.bias, decoder.up_blocks.0.resnets.1.temporal_res_block.conv1.weight, decoder.up_blocks.0.resnets.0.spatial_res_block.norm2.weight, decoder.up_blocks.1.resnets.0.time_mixer.mix_factor, decoder.up_blocks.3.resnets.0.temporal_res_block.norm1.weight, decoder.mid_block.resnets.1.spatial_res_block.conv2.bias, decoder.up_blocks.3.resnets.0.temporal_res_block.conv2.weight, decoder.up_blocks.0.resnets.2.temporal_res_block.norm1.weight, decoder.up_blocks.1.upsamplers.0.conv.weight, decoder.up_blocks.1.resnets.1.temporal_res_block.norm1.bias, decoder.up_blocks.3.resnets.0.spatial_res_block.norm1.weight, decoder.up_blocks.3.resnets.0.temporal_res_block.norm2.bias, decoder.up_blocks.2.resnets.0.spatial_res_block.conv2.bias, decoder.up_blocks.0.resnets.0.spatial_res_block.norm1.weight, decoder.up_blocks.2.resnets.0.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.1.temporal_res_block.conv1.weight, decoder.up_blocks.2.resnets.2.spatial_res_block.norm2.weight, decoder.up_blocks.0.resnets.2.temporal_res_block.conv2.bias, decoder.up_blocks.1.resnets.0.temporal_res_block.conv2.bias, decoder.mid_block.resnets.1.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.2.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.0.spatial_res_block.norm2.bias, decoder.up_blocks.1.resnets.2.spatial_res_block.norm2.bias, decoder.mid_block.resnets.0.spatial_res_block.norm1.weight, decoder.up_blocks.2.resnets.2.temporal_res_block.norm2.weight, decoder.up_blocks.3.resnets.0.spatial_res_block.conv2.weight, decoder.mid_block.resnets.0.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.1.time_mixer.mix_factor, decoder.up_blocks.0.resnets.0.temporal_res_block.norm1.weight, decoder.up_blocks.0.resnets.2.temporal_res_block.conv1.bias, decoder.up_blocks.0.resnets.1.spatial_res_block.norm1.weight, decoder.up_blocks.0.resnets.0.spatial_res_block.conv1.weight, decoder.up_blocks.2.resnets.2.spatial_res_block.conv1.weight, decoder.up_blocks.3.resnets.1.temporal_res_block.norm1.weight, decoder.up_blocks.2.resnets.0.spatial_res_block.norm2.weight, decoder.up_blocks.1.resnets.2.spatial_res_block.norm1.bias, decoder.up_blocks.2.resnets.1.spatial_res_block.norm1.weight, decoder.up_blocks.2.resnets.2.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.1.temporal_res_block.norm2.weight, decoder.mid_block.resnets.0.spatial_res_block.conv1.bias, decoder.mid_block.resnets.0.spatial_res_block.conv1.weight, decoder.up_blocks.1.resnets.1.temporal_res_block.conv2.bias, decoder.mid_block.resnets.1.spatial_res_block.norm2.weight, decoder.up_blocks.3.resnets.0.spatial_res_block.conv1.weight, decoder.up_blocks.3.resnets.2.spatial_res_block.conv1.weight, decoder.up_blocks.2.resnets.2.temporal_res_block.conv2.weight, decoder.up_blocks.1.resnets.0.spatial_res_block.conv2.weight, decoder.up_blocks.1.resnets.2.temporal_res_block.conv2.bias, decoder.up_blocks.2.resnets.0.temporal_res_block.conv2.weight, decoder.up_blocks.2.resnets.2.spatial_res_block.conv1.bias, decoder.up_blocks.3.resnets.0.spatial_res_block.norm2.bias, decoder.up_blocks.1.resnets.0.spatial_res_block.conv1.bias, decoder.mid_block.resnets.0.spatial_res_block.conv2.bias, decoder.up_blocks.3.resnets.0.spatial_res_block.conv1.bias, decoder.mid_block.resnets.1.time_mixer.mix_factor, decoder.up_blocks.1.resnets.0.spatial_res_block.conv2.bias, decoder.up_blocks.2.resnets.0.temporal_res_block.norm2.weight, decoder.up_blocks.2.resnets.1.spatial_res_block.norm2.weight, decoder.up_blocks.3.resnets.1.temporal_res_block.conv2.bias, decoder.up_blocks.0.resnets.0.temporal_res_block.conv1.bias, decoder.up_blocks.0.resnets.2.temporal_res_block.norm2.bias, decoder.mid_block.resnets.1.temporal_res_block.conv1.bias, decoder.up_blocks.2.resnets.1.spatial_res_block.conv1.weight, decoder.up_blocks.0.resnets.2.spatial_res_block.conv1.bias, decoder.up_blocks.0.resnets.0.spatial_res_block.norm1.bias, decoder.up_blocks.0.resnets.2.spatial_res_block.norm2.weight, decoder.mid_block.resnets.0.temporal_res_block.norm1.weight, decoder.up_blocks.0.resnets.1.temporal_res_block.norm1.weight, decoder.up_blocks.1.resnets.1.spatial_res_block.conv2.weight, decoder.mid_block.resnets.0.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.1.spatial_res_block.conv2.bias, decoder.up_blocks.2.resnets.2.temporal_res_block.conv1.weight, decoder.up_blocks.1.resnets.1.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.2.time_mixer.mix_factor, decoder.up_blocks.2.upsamplers.0.conv.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.norm2.weight, decoder.up_blocks.2.resnets.2.temporal_res_block.conv1.bias, decoder.up_blocks.2.resnets.2.temporal_res_block.norm2.bias, decoder.up_blocks.3.resnets.2.spatial_res_block.norm2.bias, decoder.up_blocks.3.resnets.2.temporal_res_block.norm2.weight, decoder.up_blocks.0.resnets.2.spatial_res_block.norm1.weight, decoder.up_blocks.2.resnets.2.temporal_res_block.norm1.weight, decoder.up_blocks.3.resnets.0.spatial_res_block.conv_shortcut.bias, decoder.up_blocks.2.resnets.0.time_mixer.mix_factor, decoder.mid_block.resnets.1.spatial_res_block.norm1.weight, decoder.up_blocks.2.resnets.1.spatial_res_block.conv2.bias, decoder.up_blocks.3.resnets.0.temporal_res_block.norm1.bias, decoder.up_blocks.0.resnets.0.spatial_res_block.conv2.weight, decoder.up_blocks.0.resnets.0.temporal_res_block.norm2.weight, decoder.up_blocks.3.resnets.0.temporal_res_block.conv1.weight, decoder.up_blocks.1.resnets.1.spatial_res_block.norm1.weight, decoder.up_blocks.0.resnets.2.spatial_res_block.conv2.weight, decoder.mid_block.resnets.1.temporal_res_block.norm2.bias, decoder.up_blocks.1.resnets.2.temporal_res_block.conv2.weight, decoder.mid_block.resnets.1.spatial_res_block.conv1.bias, decoder.mid_block.resnets.0.temporal_res_block.conv2.weight, decoder.up_blocks.0.resnets.0.temporal_res_block.conv2.weight, decoder.up_blocks.3.resnets.1.temporal_res_block.conv2.weight, decoder.up_blocks.0.resnets.1.spatial_res_block.conv2.weight, decoder.mid_block.resnets.0.temporal_res_block.conv1.weight, decoder.up_blocks.2.resnets.1.spatial_res_block.conv2.weight, decoder.up_blocks.1.resnets.2.spatial_res_block.conv1.bias, decoder.up_blocks.0.resnets.1.spatial_res_block.norm1.bias, decoder.up_blocks.3.resnets.2.temporal_res_block.conv2.weight, decoder.up_blocks.1.resnets.0.spatial_res_block.norm2.bias, decoder.up_blocks.3.resnets.2.spatial_res_block.norm1.weight, decoder.mid_block.resnets.1.temporal_res_block.conv2.bias, decoder.mid_block.resnets.1.spatial_res_block.conv1.weight, decoder.up_blocks.1.resnets.0.spatial_res_block.norm1.bias, decoder.up_blocks.2.resnets.2.spatial_res_block.conv2.bias, decoder.up_blocks.2.resnets.0.spatial_res_block.norm1.weight, decoder.up_blocks.3.resnets.2.spatial_res_block.conv1.bias, decoder.up_blocks.1.resnets.1.spatial_res_block.conv1.weight, decoder.up_blocks.0.resnets.2.spatial_res_block.conv2.bias, decoder.up_blocks.1.resnets.2.spatial_res_block.conv2.bias, decoder.mid_block.resnets.1.spatial_res_block.conv2.weight, decoder.up_blocks.3.resnets.2.time_mixer.mix_factor, decoder.up_blocks.0.resnets.0.temporal_res_block.norm1.bias, decoder.mid_block.resnets.0.temporal_res_block.conv2.bias, decoder.up_blocks.3.resnets.1.spatial_res_block.conv1.weight, decoder.up_blocks.3.resnets.1.temporal_res_block.norm2.weight, decoder.up_blocks.2.resnets.0.temporal_res_block.norm1.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.conv2.bias, decoder.up_blocks.1.resnets.2.spatial_res_block.norm2.weight, decoder.up_blocks.0.resnets.1.temporal_res_block.conv2.weight, decoder.up_blocks.2.resnets.0.spatial_res_block.conv_shortcut.weight, decoder.up_blocks.3.resnets.2.temporal_res_block.norm1.bias, decoder.up_blocks.1.resnets.0.temporal_res_block.norm1.bias, decoder.up_blocks.0.resnets.2.time_mixer.mix_factor, decoder.up_blocks.2.upsamplers.0.conv.bias, decoder.mid_block.resnets.0.temporal_res_block.norm1.bias, decoder.up_blocks.1.resnets.1.spatial_res_block.norm1.bias, decoder.up_blocks.1.resnets.0.temporal_res_block.norm1.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.conv1.weight, decoder.up_blocks.3.resnets.0.spatial_res_block.conv2.bias, decoder.up_blocks.0.resnets.0.time_mixer.mix_factor, decoder.up_blocks.0.resnets.2.temporal_res_block.conv1.weight, decoder.mid_block.resnets.1.temporal_res_block.conv1.weight, decoder.mid_block.resnets.0.temporal_res_block.norm2.weight, decoder.up_blocks.0.resnets.1.spatial_res_block.conv1.bias, decoder.mid_block.resnets.1.temporal_res_block.norm1.weight, decoder.up_blocks.0.resnets.2.temporal_res_block.norm2.weight, decoder.up_blocks.0.resnets.0.spatial_res_block.conv2.bias, decoder.up_blocks.3.resnets.1.temporal_res_block.conv1.bias, decoder.up_blocks.2.resnets.2.temporal_res_block.norm1.bias, decoder.up_blocks.2.resnets.2.spatial_res_block.norm1.weight, decoder.up_blocks.1.resnets.0.temporal_res_block.norm2.weight, decoder.up_blocks.1.resnets.2.spatial_res_block.conv1.weight, decoder.up_blocks.3.resnets.2.temporal_res_block.conv2.bias, decoder.up_blocks.0.resnets.2.temporal_res_block.norm1.bias, decoder.mid_block.resnets.1.temporal_res_block.norm2.weight, decoder.up_blocks.2.resnets.0.spatial_res_block.conv1.bias, decoder.up_blocks.2.resnets.0.temporal_res_block.conv1.weight, decoder.up_blocks.3.resnets.2.temporal_res_block.norm1.weight, decoder.up_blocks.3.resnets.0.spatial_res_block.norm1.bias, decoder.mid_block.resnets.0.spatial_res_block.norm1.bias, decoder.up_blocks.3.resnets.1.spatial_res_block.norm2.bias, decoder.up_blocks.2.resnets.1.temporal_res_block.norm2.bias, decoder.time_conv_out.bias, decoder.up_blocks.3.resnets.0.time_mixer.mix_factor, decoder.up_blocks.2.resnets.2.spatial_res_block.conv2.weight, decoder.up_blocks.0.resnets.0.spatial_res_block.conv1.bias, decoder.up_blocks.3.resnets.0.temporal_res_block.conv2.bias, decoder.up_blocks.3.resnets.1.spatial_res_block.norm2.weight, decoder.up_blocks.2.resnets.2.time_mixer.mix_factor, decoder.up_blocks.2.resnets.1.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.2.spatial_res_block.conv1.weight, decoder.up_blocks.1.resnets.0.spatial_res_block.norm2.weight, decoder.up_blocks.2.resnets.1.time_mixer.mix_factor, decoder.up_blocks.0.resnets.2.spatial_res_block.norm1.bias, decoder.up_blocks.2.resnets.0.spatial_res_block.conv1.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.norm1.weight, decoder.up_blocks.2.resnets.0.temporal_res_block.norm2.bias, decoder.up_blocks.3.resnets.1.time_mixer.mix_factor, decoder.up_blocks.1.resnets.0.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.0.spatial_res_block.norm1.weight, decoder.time_conv_out.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.norm1.bias, decoder.up_blocks.1.resnets.2.temporal_res_block.conv1.bias, decoder.up_blocks.3.resnets.0.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.0.temporal_res_block.conv2.weight, decoder.up_blocks.1.resnets.2.temporal_res_block.norm2.weight, decoder.up_blocks.3.resnets.2.spatial_res_block.conv2.weight, decoder.up_blocks.3.resnets.1.spatial_res_block.norm1.weight, decoder.up_blocks.3.resnets.2.spatial_res_block.norm1.bias, decoder.up_blocks.0.resnets.1.spatial_res_block.norm2.bias, decoder.up_blocks.3.resnets.2.temporal_res_block.conv1.bias, decoder.up_blocks.3.resnets.2.temporal_res_block.norm2.bias, decoder.up_blocks.2.resnets.0.spatial_res_block.conv_shortcut.bias, decoder.up_blocks.0.resnets.0.temporal_res_block.conv2.bias, decoder.mid_block.resnets.1.spatial_res_block.norm1.bias, decoder.up_blocks.3.resnets.2.spatial_res_block.conv2.bias, decoder.up_blocks.1.resnets.2.temporal_res_block.norm1.bias, decoder.up_blocks.3.resnets.0.spatial_res_block.conv_shortcut.weight, decoder.up_blocks.3.resnets.1.spatial_res_block.conv2.bias, decoder.up_blocks.0.resnets.1.spatial_res_block.conv1.weight, decoder.up_blocks.1.resnets.2.spatial_res_block.norm1.weight, decoder.up_blocks.0.resnets.1.spatial_res_block.norm2.weight, decoder.up_blocks.1.resnets.0.temporal_res_block.conv1.weight, decoder.up_blocks.2.resnets.0.spatial_res_block.norm2.bias, decoder.up_blocks.0.resnets.1.spatial_res_block.conv2.bias, decoder.mid_block.resnets.1.temporal_res_block.norm1.bias, decoder.up_blocks.3.resnets.1.spatial_res_block.conv1.bias, decoder.up_blocks.2.resnets.1.spatial_res_block.norm1.bias, decoder.up_blocks.0.resnets.1.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.2.temporal_res_block.conv1.weight, decoder.mid_block.resnets.0.temporal_res_block.norm2.bias, decoder.up_blocks.3.resnets.2.spatial_res_block.norm2.weight, decoder.mid_block.resnets.1.temporal_res_block.conv2.weight, decoder.up_blocks.3.resnets.2.temporal_res_block.conv1.weight, decoder.up_blocks.0.resnets.1.temporal_res_block.norm2.bias, decoder.up_blocks.3.resnets.0.spatial_res_block.norm2.weight, decoder.up_blocks.1.resnets.0.spatial_res_block.conv1.weight, decoder.mid_block.resnets.0.spatial_res_block.norm2.weight, decoder.up_blocks.0.resnets.0.temporal_res_block.norm2.bias, decoder.up_blocks.3.resnets.1.temporal_res_block.conv1.weight, decoder.up_blocks.1.resnets.1.temporal_res_block.norm2.bias, decoder.up_blocks.1.resnets.2.spatial_res_block.conv2.weight, decoder.up_blocks.1.resnets.1.spatial_res_block.norm2.weight, decoder.up_blocks.3.resnets.1.spatial_res_block.norm1.bias, decoder.up_blocks.3.resnets.0.temporal_res_block.norm2.weight, decoder.up_blocks.2.resnets.1.temporal_res_block.conv2.weight, decoder.up_blocks.0.resnets.1.temporal_res_block.conv2.bias, decoder.up_blocks.2.resnets.0.spatial_res_block.norm1.bias, decoder.up_blocks.2.resnets.2.spatial_res_block.norm1.bias, decoder.up_blocks.1.resnets.1.time_mixer.mix_factor, decoder.up_blocks.1.resnets.1.spatial_res_block.conv1.bias, decoder.up_blocks.2.resnets.1.temporal_res_block.conv1.bias, decoder.up_blocks.1.resnets.1.temporal_res_block.conv2.weight, decoder.up_blocks.1.resnets.1.temporal_res_block.norm2.weight, decoder.up_blocks.1.resnets.1.spatial_res_block.norm2.bias']\n",
            "Some weights of AutoencoderKL were not initialized from the model checkpoint at stabilityai/stable-video-diffusion-img2vid-xt and are newly initialized: ['decoder.mid_block.resnets.1.norm2.bias', 'decoder.up_blocks.0.resnets.1.norm1.bias', 'decoder.up_blocks.0.resnets.2.conv1.bias', 'decoder.up_blocks.0.resnets.0.conv1.bias', 'decoder.up_blocks.0.resnets.1.conv1.bias', 'decoder.mid_block.resnets.1.conv2.bias', 'decoder.up_blocks.0.resnets.1.norm2.bias', 'decoder.mid_block.resnets.0.norm1.bias', 'decoder.up_blocks.0.resnets.0.norm2.bias', 'decoder.mid_block.resnets.1.conv2.weight', 'post_quant_conv.bias', 'decoder.up_blocks.0.resnets.2.norm2.bias', 'decoder.up_blocks.0.resnets.0.norm1.bias', 'decoder.up_blocks.0.resnets.1.conv1.weight', 'decoder.mid_block.resnets.0.norm2.bias', 'decoder.mid_block.resnets.0.conv1.weight', 'post_quant_conv.weight', 'decoder.mid_block.resnets.1.norm1.weight', 'decoder.mid_block.resnets.1.norm1.bias', 'decoder.mid_block.resnets.0.conv1.bias', 'decoder.up_blocks.0.resnets.2.conv2.weight', 'decoder.up_blocks.0.resnets.2.conv1.weight', 'decoder.up_blocks.0.resnets.0.conv1.weight', 'decoder.mid_block.resnets.0.conv2.bias', 'decoder.mid_block.resnets.0.norm2.weight', 'decoder.up_blocks.0.resnets.1.norm1.weight', 'decoder.up_blocks.0.resnets.2.conv2.bias', 'decoder.up_blocks.0.resnets.0.conv2.bias', 'decoder.mid_block.resnets.1.conv1.bias', 'decoder.mid_block.resnets.0.conv2.weight', 'decoder.mid_block.resnets.0.norm1.weight', 'decoder.up_blocks.0.resnets.2.norm1.weight', 'decoder.up_blocks.0.resnets.0.norm1.weight', 'decoder.up_blocks.0.resnets.1.conv2.weight', 'decoder.mid_block.resnets.1.conv1.weight', 'decoder.up_blocks.0.resnets.1.norm2.weight', 'decoder.up_blocks.0.resnets.2.norm1.bias', 'decoder.up_blocks.0.resnets.1.conv2.bias', 'decoder.mid_block.resnets.1.norm2.weight', 'decoder.up_blocks.0.resnets.0.conv2.weight', 'decoder.up_blocks.0.resnets.2.norm2.weight', 'decoder.up_blocks.0.resnets.0.norm2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ----------------- 4. Model Loading -----------------\n",
        "\n",
        "print(\"Loading models...\")\n",
        "\n",
        "try:\n",
        "    # Load models directly to GPU with low_cpu_mem_usage=False to avoid meta device\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        subfolder=\"vae\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=False\n",
        "    ).to(device)\n",
        "\n",
        "    unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        subfolder=\"unet\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=False\n",
        "    ).to(device)\n",
        "\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        subfolder=\"image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=False\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Models loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XpniSELzdt8",
        "outputId": "44298d2a-24c1-41d9-9a99-358c140888e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SCE initialized with 256 feature dimensions\n",
            "ControlAdapter3D: 32 base -> 4 latent channels\n",
            "MPD initialized\n",
            "\n",
            "Total Trainable Parameters: 408,121\n",
            "All models ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Freeze base models\n",
        "vae.requires_grad_(False)\n",
        "unet.requires_grad_(False)\n",
        "image_encoder.requires_grad_(False)\n",
        "\n",
        "# Initialize SCE\n",
        "sce = SelectiveContentEncoder(feature_dim=256).to(device)\n",
        "\n",
        "# Initialize Control Adapter\n",
        "control_adapter = ControlAdapter3D(in_channels=1, base_channels=32, latent_channels=4).to(device)\n",
        "\n",
        "# Initialize MPD\n",
        "mpd = MaskPredictionDecoder(latent_channels=4, sce_feature_dim=256).to(device)\n",
        "\n",
        "# Trainable parameters\n",
        "sce.train()\n",
        "control_adapter.train()\n",
        "mpd.train()\n",
        "\n",
        "trainable_params = list(sce.parameters()) + list(control_adapter.parameters()) + list(mpd.parameters())\n",
        "print(f\"\\nTotal Trainable Parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "\n",
        "optimizer = AdamW(trainable_params, lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Load noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
        "\n",
        "# Prepare with accelerator\n",
        "sce, control_adapter, mpd, optimizer = accelerator.prepare(\n",
        "    sce, control_adapter, mpd, optimizer\n",
        ")\n",
        "\n",
        "# Keep frozen models in eval\n",
        "vae.eval()\n",
        "unet.eval()\n",
        "image_encoder.eval()\n",
        "\n",
        "print(\"All models ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Lzwzr2N92o5E",
        "outputId": "4d12d7ac-741c-48e2-e0cf-9d219d1cd7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Extracting data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 100%|██████████| 1203/1203 [00:01<00:00, 605.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading dataset: name 'RoVITrackingInpaintingDataset' is not defined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'RoVITrackingInpaintingDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1841840082.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Create dataset and dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoVITrackingInpaintingDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRESOLUTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRESOLUTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed class name!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RoVITrackingInpaintingDataset' is not defined"
          ]
        }
      ],
      "source": [
        "# ----------------- 5. Data Loading -----------------\n",
        "\n",
        "from google.colab import drive\n",
        "import os, zipfile\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Extract data\n",
        "zip_path = \"/content/drive/MyDrive/data-reduc.zip\"\n",
        "extract_to = \"/content/data\"\n",
        "\n",
        "print(\"Extracting data...\")\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(DATASET_ROOT):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        for file in tqdm(zip_ref.infolist(), desc=\"Extracting\"):\n",
        "            zip_ref.extract(file, extract_to)\n",
        "else:\n",
        "    print(\"Data already extracted, skipping...\")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "try:\n",
        "    dataset = RoVITrackingInpaintingDataset(DATASET_ROOT, NUM_FRAMES, (RESOLUTION, RESOLUTION))  # Changed class name!\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    dataloader = accelerator.prepare(dataloader)\n",
        "    print(f\"Dataset loaded: {len(dataset)} sequences\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM8z_FnA2o3D"
      },
      "outputs": [],
      "source": [
        "# ----------------- 6. Helper Functions -----------------\n",
        "\n",
        "def encode_image(image: torch.Tensor, image_encoder, device):\n",
        "    \"\"\"Encode image with CLIP for SVD conditioning.\"\"\"\n",
        "    # image shape: (B, C, H, W)\n",
        "    image_resized = F.interpolate(\n",
        "        image,\n",
        "        size=(224, 224),\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # Convert to float16 to match model dtype\n",
        "    image_resized = image_resized.to(dtype=torch.float16)\n",
        "    image_embeddings = image_encoder(image_resized).image_embeds\n",
        "    return image_embeddings\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_latents(video: torch.Tensor, vae: AutoencoderKL):\n",
        "    \"\"\"Convert video tensor to VAE latents.\"\"\"\n",
        "    B, C, T, H, W = video.shape\n",
        "    video_flat = video.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
        "\n",
        "    # Convert to float16 to match VAE dtype\n",
        "    video_flat = video_flat.to(dtype=torch.float16)\n",
        "\n",
        "    latents = vae.encode(video_flat).latent_dist.sample()\n",
        "    latents = latents * vae.config.scaling_factor\n",
        "    # Keep as (B, C, T, H, W) - correct order for SVD UNet\n",
        "    latents = latents.reshape(B, T, *latents.shape[1:]).permute(0, 2, 1, 3, 4)\n",
        "    return latents  # Returns (B, 4, T, H/8, W/8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivU4kNqT2vak"
      },
      "outputs": [],
      "source": [
        "# ----------------- TRAINING (Completely rewritten) -----------------\n",
        "\n",
        "def train_epoch(epoch):\n",
        "    control_adapter.train()\n",
        "    mpd.train()\n",
        "    sce.train()\n",
        "\n",
        "    total_losses = []\n",
        "    inpaint_losses = []\n",
        "    mask_losses = []\n",
        "    temporal_losses = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        with accelerator.accumulate(control_adapter, mpd, sce):\n",
        "            # Get data\n",
        "            original_video = batch[\"original_video\"].to(torch.float16)\n",
        "            original_first_frame = batch[\"original_first_frame\"].to(torch.float16)\n",
        "            first_mask = batch[\"first_mask\"].to(torch.float16)  # ONLY first mask as input\n",
        "            edited_video = batch[\"edited_video\"].to(torch.float16)  # TARGET\n",
        "            mask_video = batch[\"mask_video\"].to(torch.float16)  # TARGET (all masks)\n",
        "\n",
        "            batch_size = original_video.shape[0]\n",
        "            num_frames = original_video.shape[2]\n",
        "\n",
        "            # Encode\n",
        "            with torch.no_grad():\n",
        "                target_latents = get_latents(edited_video, vae)\n",
        "                image_embeddings = encode_image(original_first_frame, image_encoder, device).unsqueeze(1)\n",
        "                first_frame_latents = vae.encode(original_first_frame).latent_dist.sample()\n",
        "                first_frame_latents = first_frame_latents * vae.config.scaling_factor\n",
        "\n",
        "            # SCE: Encode first mask\n",
        "            first_mask_2d = first_mask.squeeze(2)  # Remove T dimension: (B, 1, 1, H, W) -> (B, 1, H, W)\n",
        "            sce_features = sce(first_mask_2d)  # (B, feature_dim)\n",
        "            # sce_features = sce(first_mask)  # (B, feature_dim)\n",
        "\n",
        "            # Add noise to target\n",
        "            noise = torch.randn_like(target_latents)\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)\n",
        "\n",
        "            # Control adapter: Use full mask video (during training we have ground truth)\n",
        "            control_cond = control_adapter(mask_video)\n",
        "            control_cond = F.interpolate(control_cond, size=noisy_latents.shape[-3:], mode='trilinear', align_corners=False)\n",
        "\n",
        "            # Add control\n",
        "            controlled_latents = noisy_latents + control_cond\n",
        "\n",
        "            # Expand first frame\n",
        "            with torch.no_grad():\n",
        "                first_frame_latents_expanded = first_frame_latents.unsqueeze(2).repeat(1, 1, num_frames, 1, 1)\n",
        "\n",
        "            # Concatenate\n",
        "            conditioned_latents = torch.cat([controlled_latents, first_frame_latents_expanded], dim=1)\n",
        "            conditioned_latents = conditioned_latents.permute(0, 2, 1, 3, 4).to(torch.float16)\n",
        "\n",
        "            # Time IDs\n",
        "            with torch.no_grad():\n",
        "                added_time_ids = torch.tensor([[7, 127, 0.02]], dtype=torch.float16, device=device).repeat(batch_size, 1)\n",
        "                image_embeddings_fp16 = image_embeddings.to(torch.float16)\n",
        "\n",
        "            # UNet forward (inpainting)\n",
        "            model_pred = unet(conditioned_latents, timesteps, encoder_hidden_states=image_embeddings_fp16,\n",
        "                            added_time_ids=added_time_ids, return_dict=False)[0]\n",
        "            model_pred = model_pred.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "            # MPD: Predict masks from controlled latents + SCE features\n",
        "            predicted_masks = mpd(controlled_latents, sce_features, num_frames)\n",
        "\n",
        "            # Upsample predicted masks to match mask_video size\n",
        "            predicted_masks_upsampled = F.interpolate(\n",
        "                predicted_masks,\n",
        "                size=mask_video.shape[-3:],\n",
        "                mode='trilinear',\n",
        "                align_corners=False\n",
        "            )\n",
        "\n",
        "            # === LOSSES ===\n",
        "\n",
        "            # 1. Inpainting loss (denoising)\n",
        "            inpaint_loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "\n",
        "            # 2. Mask prediction loss (tracking)\n",
        "            mask_loss = F.binary_cross_entropy(\n",
        "                predicted_masks_upsampled.float(),\n",
        "                mask_video.float(),\n",
        "                reduction=\"mean\"\n",
        "            )\n",
        "            # 3. Temporal consistency\n",
        "            pred_diff = torch.abs(model_pred[:, :, 1:] - model_pred[:, :, :-1])\n",
        "            temporal_loss = pred_diff.mean()\n",
        "\n",
        "            # 4. Region-aware loss (focus on masked regions)\n",
        "            denoise_loss_per_pixel = F.mse_loss(model_pred.float(), noise.float(), reduction=\"none\")\n",
        "            with torch.no_grad():\n",
        "                latent_mask = F.interpolate(mask_video, size=target_latents.shape[-3:], mode='trilinear', align_corners=False)\n",
        "\n",
        "            ra_inside = (denoise_loss_per_pixel * latent_mask).sum() / (latent_mask.sum() + 1e-6)\n",
        "            ra_outside = (denoise_loss_per_pixel * (1 - latent_mask)).sum() / ((1 - latent_mask).sum() + 1e-6)\n",
        "\n",
        "            # Total loss\n",
        "            lambda_inpaint = 1.0\n",
        "            lambda_mask = 2.0  # Mask prediction is important\n",
        "            lambda_temp = 0.2\n",
        "            lambda_ra = 0.5\n",
        "\n",
        "            total_loss = (lambda_inpaint * inpaint_loss) + \\\n",
        "                        (lambda_mask * mask_loss) + \\\n",
        "                        (lambda_temp * temporal_loss) + \\\n",
        "                        (lambda_ra * (ra_inside + ra_outside))\n",
        "\n",
        "            # Backprop\n",
        "            accelerator.backward(total_loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                accelerator.clip_grad_norm_(trainable_params, 1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Logging\n",
        "            total_losses.append(total_loss.item())\n",
        "            inpaint_losses.append(inpaint_loss.item())\n",
        "            mask_losses.append(mask_loss.item())\n",
        "            temporal_losses.append(temporal_loss.item())\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'total': f\"{total_loss.item():.4f}\",\n",
        "                'inpaint': f\"{inpaint_loss.item():.4f}\",\n",
        "                'mask': f\"{mask_loss.item():.4f}\",\n",
        "                'temp': f\"{temporal_loss.item():.4f}\"\n",
        "            })\n",
        "\n",
        "            if step % 5 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'total': np.mean(total_losses),\n",
        "        'inpaint': np.mean(inpaint_losses),\n",
        "        'mask': np.mean(mask_losses),\n",
        "        'temporal': np.mean(temporal_losses)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Oz7lvxYRrq",
        "outputId": "be08987d-8bd2-4217-a447-1f1c4c1f45aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Training for 10 Epochs\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 10/10 [00:20<00:00,  2.10s/it, total=21.0448, inpaint=3.8780, mask=0.6217, temp=0.0917]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 1/10 Summary\n",
            "============================================================\n",
            "Total Loss:     15.0342\n",
            "Inpaint Loss:   2.7578\n",
            "Mask Loss:      0.6387\n",
            "Temporal Loss:  0.1629\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 10/10 [00:21<00:00,  2.14s/it, total=17.8813, inpaint=3.2598, mask=0.6485, temp=0.1039]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 2/10 Summary\n",
            "============================================================\n",
            "Total Loss:     14.9526\n",
            "Inpaint Loss:   2.6988\n",
            "Mask Loss:      0.6351\n",
            "Temporal Loss:  0.1713\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to checkpoints/epoch_2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 10/10 [00:20<00:00,  2.09s/it, total=12.3363, inpaint=2.1894, mask=0.6429, temp=0.1569]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 3/10 Summary\n",
            "============================================================\n",
            "Total Loss:     15.6775\n",
            "Inpaint Loss:   2.8569\n",
            "Mask Loss:      0.6337\n",
            "Temporal Loss:  0.1479\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 10/10 [00:20<00:00,  2.06s/it, total=9.9912, inpaint=1.8634, mask=0.6260, temp=0.2063]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 4/10 Summary\n",
            "============================================================\n",
            "Total Loss:     13.6095\n",
            "Inpaint Loss:   2.5058\n",
            "Mask Loss:      0.6310\n",
            "Temporal Loss:  0.1618\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to checkpoints/epoch_4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 10/10 [00:20<00:00,  2.08s/it, total=17.6049, inpaint=2.7859, mask=0.6260, temp=0.0710]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 5/10 Summary\n",
            "============================================================\n",
            "Total Loss:     15.7320\n",
            "Inpaint Loss:   2.8388\n",
            "Mask Loss:      0.6276\n",
            "Temporal Loss:  0.1248\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 10/10 [00:20<00:00,  2.10s/it, total=17.6415, inpaint=3.2491, mask=0.6138, temp=0.1295]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 6/10 Summary\n",
            "============================================================\n",
            "Total Loss:     15.7318\n",
            "Inpaint Loss:   2.8949\n",
            "Mask Loss:      0.6272\n",
            "Temporal Loss:  0.1452\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to checkpoints/epoch_6\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 10/10 [00:20<00:00,  2.08s/it, total=11.7046, inpaint=2.1264, mask=0.6174, temp=0.2429]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 7/10 Summary\n",
            "============================================================\n",
            "Total Loss:     14.9527\n",
            "Inpaint Loss:   2.7418\n",
            "Mask Loss:      0.6236\n",
            "Temporal Loss:  0.1477\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 10/10 [00:20<00:00,  2.08s/it, total=18.5052, inpaint=3.3762, mask=0.6388, temp=0.0776]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 8/10 Summary\n",
            "============================================================\n",
            "Total Loss:     16.3070\n",
            "Inpaint Loss:   2.9740\n",
            "Mask Loss:      0.6207\n",
            "Temporal Loss:  0.1404\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to checkpoints/epoch_8\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 10/10 [00:20<00:00,  2.09s/it, total=14.3713, inpaint=2.7069, mask=0.6028, temp=0.1263]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 9/10 Summary\n",
            "============================================================\n",
            "Total Loss:     13.8158\n",
            "Inpaint Loss:   2.5498\n",
            "Mask Loss:      0.6171\n",
            "Temporal Loss:  0.1799\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 10/10 [00:20<00:00,  2.09s/it, total=13.6323, inpaint=2.8993, mask=0.5993, temp=0.2070]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EPOCH 10/10 Summary\n",
            "============================================================\n",
            "Total Loss:     16.0338\n",
            "Inpaint Loss:   2.9509\n",
            "Mask Loss:      0.6149\n",
            "Temporal Loss:  0.1499\n",
            "============================================================\n",
            "\n",
            "Checkpoint saved to checkpoints/epoch_10\n",
            "\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# ----------------- 8. Main Training Loop -----------------\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting Training for {NUM_EPOCHS} Epochs\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for epoch in range(10):\n",
        "    losses = train_epoch(epoch + 1)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EPOCH {epoch+1}/{NUM_EPOCHS} Summary\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total Loss:     {losses['total']:.4f}\")\n",
        "    print(f\"Inpaint Loss:   {losses['inpaint']:.4f}\")  # Changed\n",
        "    print(f\"Mask Loss:      {losses['mask']:.4f}\")      # Changed\n",
        "    print(f\"Temporal Loss:  {losses['temporal']:.4f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % CHECKPOINT_SAVE_EPOCHS == 0:\n",
        "        checkpoint_dir = Path(f\"./checkpoints/epoch_{epoch+1}\")\n",
        "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save with accelerator\n",
        "        accelerator.save_state(str(checkpoint_dir))\n",
        "        print(f\"Checkpoint saved to {checkpoint_dir}\\n\")\n",
        "\n",
        "        # Clear memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "tsFRmqaRJdU9",
        "outputId": "14d74337-5586-4699-f450-cced6ad6b5ab",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/bird.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-704625099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/bird.zip'"
          ]
        }
      ],
      "source": [
        "# Extract data\n",
        "zip_path = \"/content/bird.zip\"\n",
        "extract_to = \"/content/\"\n",
        "\n",
        "print(\"Extracting data...\")\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    for file in tqdm(zip_ref.infolist(), desc=\"Extracting\"):\n",
        "        zip_ref.extract(file, extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zml1smlaJ-pu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PETzelVSqDc"
      },
      "outputs": [],
      "source": [
        "# Load SD 1.5 VAE for decoding\n",
        "print(\"Loading Stable Diffusion VAE for decoding...\")\n",
        "sd_vae = AutoencoderKL.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    subfolder=\"vae\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=False\n",
        ").to(device)\n",
        "sd_vae.eval()\n",
        "print(\"✓ SD VAE loaded!\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_latents_with_sd_vae(latents, sd_vae):\n",
        "    \"\"\"Decode video latents frame by frame using SD VAE.\"\"\"\n",
        "    B, C, T, H, W = latents.shape\n",
        "    decoded_frames = []\n",
        "\n",
        "    for t in range(T):\n",
        "        # Get single frame\n",
        "        frame_latent = latents[:, :, t, :, :] / sd_vae.config.scaling_factor\n",
        "        frame_latent = frame_latent.to(torch.float16)\n",
        "\n",
        "        # Decode\n",
        "        decoded = sd_vae.decode(frame_latent).sample\n",
        "        decoded_frames.append(decoded)\n",
        "\n",
        "    # Stack: (B, T, 3, H, W)\n",
        "    return torch.stack(decoded_frames, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSU8jhAkzvlz"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def track_and_inpaint(video_folder, first_mask_path, num_frames=6, num_inference_steps=25):\n",
        "    \"\"\"\n",
        "    Track object + inpaint video.\n",
        "    Input: Original video + ONLY first frame mask\n",
        "    Output: Inpainted video + Predicted masks for all frames\n",
        "    \"\"\"\n",
        "\n",
        "    sce.eval()\n",
        "    control_adapter.eval()\n",
        "    mpd.eval()\n",
        "    unet.eval()\n",
        "    image_encoder.eval()\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TRACKING + INPAINTING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load original video\n",
        "    print(\"\\n1. Loading original video...\")\n",
        "    video_path = Path(video_folder)\n",
        "    frame_files = sorted(list(video_path.glob('*.jpg')) + list(video_path.glob('*.png')))[:num_frames]\n",
        "\n",
        "    original_frames = []\n",
        "    for frame_file in frame_files:\n",
        "        img = Image.open(frame_file).convert('RGB').resize((RESOLUTION, RESOLUTION))\n",
        "        original_frames.append(np.array(img))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
        "    ])\n",
        "\n",
        "    original_first = Image.fromarray(original_frames[0])\n",
        "    original_first_tensor = transform(original_first).unsqueeze(0).to(device).to(torch.float16)\n",
        "\n",
        "    # Load ONLY first frame mask\n",
        "    print(\"\\n2. Loading first frame mask ONLY...\")\n",
        "    first_mask = Image.open(first_mask_path).convert('L').resize((RESOLUTION, RESOLUTION))\n",
        "    first_mask_array = np.array(first_mask) / 255.0\n",
        "    first_mask_tensor = torch.from_numpy(first_mask_array).unsqueeze(0).unsqueeze(0).to(device).to(torch.float16)\n",
        "    print(f\"   First mask shape: {first_mask_tensor.shape}\")\n",
        "\n",
        "    # Encode first frame\n",
        "    print(\"\\n3. Encoding original first frame...\")\n",
        "    image_embeddings = encode_image(original_first_tensor, image_encoder, device).unsqueeze(1)\n",
        "    first_frame_latents = vae.encode(original_first_tensor).latent_dist.sample()\n",
        "    first_frame_latents = first_frame_latents * vae.config.scaling_factor\n",
        "\n",
        "    # SCE: Encode first mask\n",
        "    print(\"\\n4. Encoding first mask with SCE...\")\n",
        "    sce_features = sce(first_mask_tensor)  # (B, feature_dim)\n",
        "    print(f\"   SCE features shape: {sce_features.shape}\")\n",
        "\n",
        "    # Initialize latents\n",
        "    print(\"\\n5. Initializing latents...\")\n",
        "    B, C, T = 1, 4, num_frames\n",
        "    H, W = RESOLUTION // 8, RESOLUTION // 8\n",
        "    latents = torch.randn(B, C, T, H, W, device=device, dtype=torch.float16)\n",
        "\n",
        "    noise_scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "    first_frame_latents_expanded = first_frame_latents.unsqueeze(2).repeat(1, 1, T, 1, 1)\n",
        "\n",
        "    # Denoising loop with mask prediction\n",
        "    print(f\"\\n6. Tracking + Inpainting ({num_inference_steps} steps)...\")\n",
        "\n",
        "    predicted_masks_final = None\n",
        "\n",
        "    for i, t in enumerate(tqdm(noise_scheduler.timesteps, desc=\"Tracking & Inpainting\")):\n",
        "        # MPD: Predict masks for ALL frames from current latents + SCE\n",
        "        predicted_masks = mpd(latents, sce_features, num_frames)  # (B, 1, T, H, W) in latent space\n",
        "\n",
        "        # Save final predicted masks\n",
        "        if i == len(noise_scheduler.timesteps) - 1:\n",
        "            predicted_masks_final = predicted_masks.clone()\n",
        "\n",
        "        # Control adapter: Use predicted masks (not ground truth!)\n",
        "        control_cond = control_adapter(predicted_masks)\n",
        "        control_cond = F.interpolate(\n",
        "            control_cond,\n",
        "            size=latents.shape[-3:],\n",
        "            mode='trilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Add control\n",
        "        latents_with_control = latents + control_cond\n",
        "\n",
        "        # Concatenate with first frame\n",
        "        latent_model_input = torch.cat([latents_with_control, first_frame_latents_expanded], dim=1)\n",
        "        latent_model_input = latent_model_input.permute(0, 2, 1, 3, 4).to(torch.float16)\n",
        "\n",
        "        timestep = torch.tensor([t], device=device, dtype=torch.long)\n",
        "        added_time_ids = torch.tensor([[7, 127, 0.02]], dtype=torch.float16, device=device)\n",
        "\n",
        "        # UNet prediction\n",
        "        noise_pred = unet(\n",
        "            latent_model_input,\n",
        "            timestep,\n",
        "            encoder_hidden_states=image_embeddings.to(torch.float16),\n",
        "            added_time_ids=added_time_ids,\n",
        "            return_dict=False\n",
        "        )[0]\n",
        "\n",
        "        noise_pred = noise_pred.permute(0, 2, 1, 3, 4)\n",
        "        latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "    # Decode inpainted video\n",
        "    print(\"\\n7. Decoding inpainted video...\")\n",
        "    decoded_video = decode_latents_with_sd_vae(latents, sd_vae)\n",
        "\n",
        "    inpainted_frames = []\n",
        "    for t in range(T):\n",
        "        frame = decoded_video[0, t].cpu()\n",
        "        frame = (frame * 0.5 + 0.5).clamp(0, 1)\n",
        "        frame = frame.permute(1, 2, 0).numpy().astype(np.float32)\n",
        "        inpainted_frames.append(frame)\n",
        "\n",
        "    # Upsample predicted masks for visualization\n",
        "    print(\"\\n8. Upsampling predicted masks...\")\n",
        "    predicted_masks_upsampled = F.interpolate(\n",
        "        predicted_masks_final,\n",
        "        size=(num_frames, RESOLUTION, RESOLUTION),\n",
        "        mode='trilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "    predicted_masks_np = predicted_masks_upsampled[0, 0].cpu().numpy()  # (T, H, W)\n",
        "\n",
        "    print(\"   ✓ Complete!\")\n",
        "\n",
        "    # Visualize\n",
        "    print(\"\\n9. Visualizing...\")\n",
        "    fig, axes = plt.subplots(3, num_frames, figsize=(20, 12))\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        # Row 1: Original (with object)\n",
        "        axes[0, i].imshow(original_frames[i])\n",
        "        axes[0, i].set_title(f'Original {i}', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Row 2: Predicted masks (tracking!)\n",
        "        axes[1, i].imshow(predicted_masks_np[i], cmap='gray', vmin=0, vmax=1)\n",
        "        if i == 0:\n",
        "            axes[1, i].set_title(f'Mask {i} (Input)', color='green', fontsize=10, fontweight='bold')\n",
        "        else:\n",
        "            axes[1, i].set_title(f'Mask {i} (Tracked)', color='blue', fontsize=10, fontweight='bold')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "        # Row 3: Inpainted (object removed)\n",
        "        axes[2, i].imshow(inpainted_frames[i])\n",
        "        axes[2, i].set_title(f'Inpainted {i}', color='green', fontsize=10, fontweight='bold')\n",
        "        axes[2, i].axis('off')\n",
        "\n",
        "    plt.suptitle('Original | Predicted Masks (Tracking) | Inpainted (Object Removed)', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('tracking_inpainting_result.png', dpi=150, bbox_inches='tight')\n",
        "\n",
        "    from IPython.display import Image as IPImage, display\n",
        "    plt.close()\n",
        "    display(IPImage('tracking_inpainting_result.png'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ TRACKING + INPAINTING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Row 1: Original video (with cat)\")\n",
        "    print(\"Row 2: Mask 0 = Your input | Masks 1-5 = Model tracked!\")\n",
        "    print(\"Row 3: Cat removed using predicted masks\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Run inference\n",
        "VIDEO_FOLDER = \"/content/__KkKB4wzrY\"  # Original video WITH cat\n",
        "FIRST_MASK = \"/content/mask_bird.png\"     # ONLY first frame mask\n",
        "\n",
        "track_and_inpaint(VIDEO_FOLDER, FIRST_MASK, num_frames=6, num_inference_steps=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Nzye4n_zvjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d849190b-fd97-4160-8548-4b35eee5e584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "# Copy checkpoint to your mounted Drive\n",
        "!cp -r ./checkpoints/epoch_10 /content/drive/MyDrive/genprop_checkpoints/\n",
        "\n",
        "print(\"Checkpoint saved to Google Drive!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}